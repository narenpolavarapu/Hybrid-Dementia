{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d72db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "parent_dir = '/Users/saikarthik/Desktop/soft computing project /Alzheimer_MRI_4_classes_dataset'\n",
    "\n",
    "\n",
    "categories = os.listdir(parent_dir)\n",
    "\n",
    "\n",
    "category_images = {}\n",
    "category_labels = {}\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    category_dir = os.path.join(parent_dir, category)\n",
    "    \n",
    "\n",
    "    if os.path.isdir(category_dir) and category != '.DS_Store':\n",
    "        all_files = os.listdir(category_dir)\n",
    "        image_files = [filename for filename in all_files if filename.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        images = []  # An empty list to store the processed images for the current category\n",
    "        labels = [category] * len(image_files)\n",
    "        \n",
    "\n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(category_dir, image_file)\n",
    "            image = cv2.imread(image_path)\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Converting to RGB format\n",
    "            resized_image = cv2.resize(image_rgb, (224, 224))  # Resizing the image to a 224x224 pixel size\n",
    "            images.append(resized_image)\n",
    "    \n",
    "        category_images[category] = np.array(images)\n",
    "        category_labels[category] = np.array(labels)\n",
    "\n",
    "        print(f\"Shape of images for category '{category}': {category_images[category].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fbdf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for category in categories:\n",
    "    if category not in category_images:\n",
    "        continue\n",
    "    \n",
    "    sample_images = category_images[category][:10]  # Taking the first 10 images as a sample\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(len(sample_images)):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(sample_images[i])\n",
    "        plt.title(f\"Image {i+1}\")\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Category: {category}\", fontsize=14)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d09f97",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec63cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for category in category_images:\n",
    "    category_images[category] = category_images[category] / 255.0\n",
    "\n",
    "\n",
    "min_value = np.min(category_images[category])\n",
    "max_value = np.max(category_images[category])\n",
    "\n",
    "\n",
    "print(f\"Minimum Pixel Value: {min_value}\")\n",
    "print(f\"Maximum Pixel Value: {max_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c2976",
   "metadata": {},
   "source": [
    "# Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05bba2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,  # Rotating the image by up to 20 degrees\n",
    "    width_shift_range=0.2,  # Shifting the width by up to 20% of the image width\n",
    "    height_shift_range=0.2,  # Shifting the height by up to 20% of the image height\n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.2,  # Zoom in by up to 20%\n",
    "    horizontal_flip=True,  # Fliping the image horizontally\n",
    "    fill_mode='nearest'  # Fill in new pixels with the nearest existing pixel\n",
    ")\n",
    "\n",
    "output_dir = '/Users/saikarthik/Desktop/soft computing project /Alzheimer_MRI_4_classes_dataset'  # Changing to our desired output directory\n",
    "\n",
    "# Creating the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Looping through each category and apply data augmentation\n",
    "for category in categories:\n",
    "    if category not in category_images:\n",
    "        continue\n",
    "\n",
    "    images = category_images[category]\n",
    "    category_output_dir = os.path.join(output_dir, category)\n",
    "    if not os.path.exists(category_output_dir):\n",
    "        os.makedirs(category_output_dir)\n",
    "    i = 0\n",
    "    for batch in datagen.flow(images, batch_size=1):\n",
    "        augmented_image = batch[0]\n",
    "        augmented_image = (augmented_image * 255).astype(np.uint8)  \n",
    "        filename = f'{i}.jpg' \n",
    "        save_path = os.path.join(category_output_dir, filename)\n",
    "        cv2.imwrite(save_path, cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))\n",
    "        i += 1\n",
    "        if i >= 100:  \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4afb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# removing the top classification layers\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)\n",
    "\n",
    "\n",
    "parent_dir = '/Users/saikarthik/Desktop/soft computing project /Alzheimer_MRI_4_classes_dataset'\n",
    "\n",
    "# List of  all subdirectories \n",
    "categories = os.listdir(parent_dir)\n",
    "\n",
    "\n",
    "category_features = {}\n",
    "category_labels = {}\n",
    "\n",
    "# Looping through each subdirectory\n",
    "for category in categories:\n",
    "    category_dir = os.path.join(parent_dir, category)\n",
    "    \n",
    "\n",
    "    if os.path.isdir(category_dir) and category != '.DS_Store':\n",
    "        all_files = os.listdir(category_dir)\n",
    "        \n",
    "        image_files = [filename for filename in all_files if filename.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # Initializing lists to store features and labels for the category\n",
    "        features = []  \n",
    "        labels = [category] * len(image_files)\n",
    "        \n",
    "    \n",
    "        for image_file in image_files:\n",
    "            image_path = os.path.join(category_dir, image_file)\n",
    "            image = cv2.imread(image_path)\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "            resized_image = cv2.resize(image_rgb, (224, 224))  \n",
    "            preprocessed_image = preprocess_input(np.expand_dims(resized_image, axis=0))  \n",
    "            features.append(feature_extractor.predict(preprocessed_image)[0])  \n",
    "        \n",
    "\n",
    "        category_features[category] = np.array(features)\n",
    "        category_labels[category] = np.array(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb70ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features and labels into a single dataset\n",
    "all_features = np.concatenate(list(category_features.values()))\n",
    "all_labels = np.concatenate(list(category_labels.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c643d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all_features.reshape(all_features.shape[0], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9588fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x = scaler.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d549dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bed9240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ff6c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0a4d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "num_classes=4\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fdc7a",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5243864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from deap import base, creator, tools\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf_logger = tf.get_logger()\n",
    "tf_logger.setLevel(logging.ERROR)  \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_ann_model(learning_rate=0.01, hidden_units=64):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, input_dim=x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Output layer for classification\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "ann_model = create_ann_model()\n",
    "ann_model.fit(x_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "loss, accuracy = ann_model.evaluate(x_test, y_test)\n",
    "print(f'ANN Accuracy: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "if 'FitnessMax' not in creator.__dict__:\n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "\n",
    "if 'Individual' not in creator.__dict__:\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "def evaluate(individual):\n",
    "    learning_rate = individual[0]\n",
    "    hidden_units = individual[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, input_dim=x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=0, validation_data=(x_test, y_test_categorical))\n",
    "    \n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    y_pred = np.argmax(model.predict(x_test), axis=-1)\n",
    "    y_true = np.argmax(y_test, axis=-1)\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    tf.keras.backend.clear_session()  \n",
    "    return (accuracy, loss, precision, recall, f1)\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"learning_rate\", np.random.uniform, 0.001, 0.1)\n",
    "toolbox.register(\"hidden_units\", np.random.randint, 16, 128)\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,(toolbox.learning_rate, toolbox.hidden_units), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "best_accuracies = []\n",
    "losses = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "population = toolbox.population(n=20)  \n",
    "ngen = 20 \n",
    "for gen in range(ngen):\n",
    "    offspring = toolbox.select(population, len(population))\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "    \n",
    "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "        if np.random.rand() < 0.5:\n",
    "            toolbox.mate(child1, child2)\n",
    "            del child1.fitness.values\n",
    "            del child2.fitness.values\n",
    "            \n",
    "    for mutant in offspring:\n",
    "        if np.random.rand() < 0.2:\n",
    "            toolbox.mutate(mutant)\n",
    "            del mutant.fitness.values\n",
    "            \n",
    "    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "    fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "    for ind, fit in zip(invalid_ind, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    population[:] = offspring\n",
    "\n",
    "    fits = [ind.fitness.values for ind in population]\n",
    "    \n",
    "    best_accuracy, best_loss, best_precision, best_recall, best_f1 = max(fits, key=lambda x: x[0])\n",
    "    best_accuracies.append(best_accuracy)\n",
    "    losses.append(best_loss)\n",
    "    precisions.append(best_precision)\n",
    "    recalls.append(best_recall)\n",
    "    f1_scores.append(best_f1)\n",
    "\n",
    "    print(f\"Generation {gen+1}/{ngen}, Best Accuracy: {best_accuracy:.4f}, Best Loss: {best_loss:.4f}, \"\n",
    "          f\"Precision: {best_precision:.4f}, Recall: {best_recall:.4f}, F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Best Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, ngen + 1), best_accuracies, marker='o', linestyle='-', color='b', label='Best Accuracy')\n",
    "plt.title('Best Accuracy Across Generations')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(ticks=range(1, ngen + 1))  \n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, ngen + 1), losses, marker='o', linestyle='-', color='r', label='Best Loss')\n",
    "plt.title('Loss Across Generations')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(ticks=range(1, ngen + 1))  \n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_idx = np.argmax([ind.fitness.values[0] for ind in population])\n",
    "best_ind = population[best_idx]\n",
    "print(\"Best Individual: Learning Rate = {:.4f}, Hidden Units = {}\".format(best_ind[0], best_ind[1]))\n",
    "print(\"Best Fitness: {:.2f}\".format(fits[best_idx][0]))\n",
    "\n",
    "optimized_ann_model = Sequential()\n",
    "optimized_ann_model.add(Dense(best_ind[1], input_dim=x_train.shape[1], activation='relu'))\n",
    "optimized_ann_model.add(Dense(num_classes, activation='softmax'))\n",
    "optimized_ann_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=best_ind[0]), \n",
    "                            loss='categorical_crossentropy', \n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', \n",
    "                               patience=5,  \n",
    "                               restore_best_weights=True)  \n",
    "\n",
    "history = optimized_ann_model.fit(x_train, y_train, \n",
    "                                   epochs=50, \n",
    "                                   batch_size=32, \n",
    "                                   validation_split=0.2,\n",
    "                                   callbacks=[early_stopping])  \n",
    "\n",
    "loss, accuracy = optimized_ann_model.evaluate(x_test, y_test)\n",
    "print(f'Optimized ANN Accuracy: {accuracy:.2f}')\n",
    "\n",
    "y_pred = np.argmax(optimized_ann_model.predict(x_test), axis=-1)\n",
    "y_true = np.argmax(y_test, axis=-1)\n",
    "\n",
    "final_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "final_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "final_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "conf_matrix=confusion_matrix(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f'Final Precision: {final_precision:.2f}, Final Recall: {final_recall:.2f}, Final F1 Score: {final_f1:.2f}')\n",
    "\n",
    "optimized_ann_model.save('optimized_ann_model.h5')\n",
    "print(\"Model saved as 'optimized_ann_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab788f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = optimized_ann_model.evaluate(x_test, y_test)\n",
    "print(f'Optimized ANN Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfe59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "x_combined = np.concatenate((x_train, x_test), axis=0)\n",
    "\n",
    "y_combined = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "y_combined = np.argmax(y_combined, axis=1)\n",
    "\n",
    "num_features = x_combined.shape[1]  \n",
    "feature_columns = [f'feature_{i}' for i in range(num_features)]\n",
    "\n",
    "df_combined = pd.DataFrame(x_combined, columns=feature_columns)\n",
    "\n",
    "df_combined['label'] = y_combined\n",
    "\n",
    "print(df_combined.head())\n",
    "\n",
    "df_combined.to_csv('dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ffd0d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35e8dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5559b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
